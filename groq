#!/usr/bin/env perl

#-------------------------------------------------------------------------------
# This script provides command-line access to the Groq API for chat
# completions. It can accept input as a parameter or from standard input,
# allowing it to be used in a pipeline.
#
# It uses only modules from the standard library, so it requires no external
# dependencies. It should function on any system with a complete Perl v5.14
# installation.
#
# It optionally uses glow to format its output in an interactive terminal. If
# glow is not installed, will output markdown-formatted text.
#-------------------------------------------------------------------------------

use strict;
use warnings;
use v5.14;

use Digest::SHA     qw(sha256_hex);
use File::Path      qw(make_path);
use Getopt::Long    qw(GetOptions);
use HTTP::Tiny      qw();
use JSON::PP        qw(decode_json encode_json);
use Pod::Usage      qw(pod2usage);
use Term::ANSIColor qw(:constants);

#-------------------------------------------------------------------------------
# Constants
#-------------------------------------------------------------------------------
use constant DEFAULT_ROUNDS     => 3;
use constant DEFAULT_MODEL      => "llama-3.3-70b-versatile";
use constant API_MODELS         => "https://api.groq.com/openai/v1/models";
use constant API_COMPLETIONS    => "https://api.groq.com/openai/v1/chat/completions";
use constant THOUGHT_MAX_TOKENS => 2_000;
use constant SAVE_DIR           => "$ENV{HOME}/.groq/conversations";

#-------------------------------------------------------------------------------
# Command-line options
#-------------------------------------------------------------------------------
my $help;
my $query;
my $model = DEFAULT_MODEL;
my $rounds = DEFAULT_ROUNDS;
my @files;
my $save;
my $conversation;
my $list_models;
my $list_conversations;

GetOptions(
  'h|help'        => \$help,
  'q|query=s'     => \$query,
  'm|model=s'     => \$model,
  'r|rounds=i'    => \$rounds,
  'f|file=s'      => \@files,
  's|save'        => \$save,
  'c|cont=s'      => \$conversation,
  'models'        => \$list_models,
  'conversations' => \$list_conversations,
) or pod2usage(2);

pod2usage(-exitval => 1, -verbose => 2)
  if $help;

#-------------------------------------------------------------------------------
# Validation, initialization, and configuration
#-------------------------------------------------------------------------------
my $command =
    $list_models        ? 'list_models'
  : $list_conversations ? 'list_conversations'
                        : 'answer';

pod2usage("--query is required", -exitval => 2)
  if !$query               # ...query is not set
  && -t                    # ...and we are connected to a terminal
  && $command eq 'answer'; # ...but only for the 'answer' command

for (@files) {
  -e $_ || die "File not found: $_\n";
}

pod2usage("--rounds must be at least 1", -exitval => 2)
  if $rounds < 1;

my $API_KEY = $ENV{GROQ_API_KEY}
  or pod2usage("GROQ_API_KEY must be set", -exitval => 2);

# Prevents groq from formatting LLM output with glow when not connected to a
# terminal or if glow is not installed.
my $USE_GLOW =
  -t STDOUT
  && system('which glow > /dev/null 2>&1') == 0;

# Set UTF-8 encoding for STDOUT since LLMs like their emojis
binmode STDOUT, ':encoding(UTF-8)';
binmode STDERR, ':encoding(UTF-8)';

#-------------------------------------------------------------------------------
# GROQ API and model configuration
#-------------------------------------------------------------------------------
my $THINK_PROMPT = qq{
  You are an AI assistant that reasons through problems step by step.
  **Before answering, you must think inside <think>...</think> tags.**
  Do not finalize your response until explicitly instructed.
  Start by considering the user's query.
  Are there assumptions or falacies implicit in the query?
  If so, identify them and gently correct the user.
};

my $KEEP_THINKING_PROMPT = qq{
  Continue thinking.
};

my $FINALIZE_PROMPT = q{
  **Do not think any further.**
  Finalize your response.
  Format your response using markdown.
};

#-------------------------------------------------------------------------------
# HTTP setup
#-------------------------------------------------------------------------------
my $HEADERS = {
  'Content-Type'  => 'application/json',
  'Authorization' => "Bearer $API_KEY",
};

my $HTTP = HTTP::Tiny->new(
  keep_alive      => 1,
  default_headers => $HEADERS,
);

#-------------------------------------------------------------------------------
# Output Functions
#-------------------------------------------------------------------------------
sub print_thought {
  my $thought = shift;
  print STDERR BOLD ITALIC "Thinking...", RESET, "\n\n";
  print STDERR FAINT $thought, RESET, "\n\n";
}

sub print_msg {
  my $msg = shift // return;
  $USE_GLOW ? glow($msg) : print("$msg");
}

sub glow {
  my $msg = shift // return;
  open my $pipe, '|-', 'glow' or die "Failed to open pipe to glow: $!";
  binmode $pipe, ':encoding(UTF-8)';
  print $pipe $msg;
  close $pipe;
  return;
}

#-------------------------------------------------------------------------------
# Message types
#-------------------------------------------------------------------------------
sub sys_msg { {role => 'system',    content => $_[0]} }
sub ast_msg { {role => 'assistant', content => $_[0]} }
sub usr_msg { {role => 'user',      content => $_[0]} }

#-------------------------------------------------------------------------------
# API Functions
#-------------------------------------------------------------------------------
sub get  { $HTTP->get(@_) }
sub post { $HTTP->post(@_) }

sub get_models {
  my $res = get API_MODELS;

  die "$res->{status} $res->{reason}"
    unless $res->{success};

  my $data = decode_json $res->{content};

  sort map{ $_->{id} } @{$data->{data}};
}

sub get_completion {
  my ($msgs, %args) = @_;

  my $payload = encode_json{
    model    => $model,
    messages => $msgs,
    %args,
  };

  my $res = post API_COMPLETIONS, {content => $payload};

  unless ($res->{success}) {
    warn "Error: $res->{status} $res->{reason}\n";
    warn "Response: $res->{content}\n";
    die "Failed to get completion\n";
  }

  my $data = decode_json $res->{content};
  return $data->{choices}[0]{message}{content};
}

sub think {
  my $msgs  = shift;

  my $res = get_completion $msgs,
    max_completion_tokens => THOUGHT_MAX_TOKENS;

  $res =~ s/^\s*<think>//s;
  $res =~ s/<\/think>.*\z//s;
  $res =~ s/^\s+//;
  $res =~ s/\s+\z//;

  return $res;
}

sub consider {
  my $msgs = shift // [];

  unshift @$msgs, sys_msg $THINK_PROMPT;
  push @$msgs, file_msgs();
  push @$msgs, usr_msg $query;

  for (my $round = 1; $round <= $rounds; ++$round) {
    my $thought = think $msgs;
    push @$msgs, ast_msg "<think>$thought</think>";

    if ($round != $rounds) {
      push @$msgs, sys_msg $KEEP_THINKING_PROMPT;
    }

    print_thought $thought;
  }

  push @$msgs, sys_msg $FINALIZE_PROMPT;
  my $answer = get_completion $msgs;
  print_msg $answer;

  push @$msgs, ast_msg $answer;
  return $msgs;
}

sub file_msgs {
  my @msgs;

  for my $file (@files) {
    my $contents = do {
      local $/;
      open my $fh, '<', $file or die "Can't open $file: $!";
      <$fh>;
    };

    push @msgs, usr_msg qq{
      # File: $file
      ```
      $contents
      ```
    };
  }

  @msgs;
}

sub save {
  my $msgs = shift;
  my @msgs = grep{ $_->{role} =~ /user|assistant/ } @$msgs;
  my $json = encode_json \@msgs;
  my $id   = $conversation // substr sha256_hex($json), 0, 8;
  my $file = SAVE_DIR . "/$id.json";

  make_path SAVE_DIR;

  open my $fh, '>', $file
    or die "error saving conversation to $file: $!";

  print $fh $json;
  close $fh;

  return $id;
}

sub load {
  my $id   = shift;
  my $path = SAVE_DIR . "/$id.json";

  die "Conversation not found: $id\n"
    unless -e $path;

  my $json = do{
    local $/;
    open my $fh, '<', $path or die $!;
    <$fh>
  };

  decode_json $json;
}

#-------------------------------------------------------------------------------
# Commands
#-------------------------------------------------------------------------------
sub list_models {
  print "$_\n" for get_models;
}

sub list_conversations {
  my @convos = glob SAVE_DIR . '/*.json';
  for (@convos) {
    s/.*\///;
    s/\.json//;
    print "$_\n";
  }
}

sub answer {
  if (defined $query) {
    my $msgs = consider;

    if ($save) {
      my $id = save $msgs;
      print "Saved conversation: $id\n";
    }
  }
  elsif (! -t STDIN) {
    while (defined($query = <STDIN>)) {
      chomp $query;
      last if $query eq '';
      consider;
    }
  }
}

sub continue_conversation {
  my $msgs = load $conversation;
  my $updated_msgs = consider $msgs;

  if ($save) {
    my $id = save $updated_msgs;
    print "Saved continued conversation: $id\n";
  }
}

#-------------------------------------------------------------------------------
# Main
#-------------------------------------------------------------------------------
  $command eq 'list_models'         ? list_models
: $command eq 'list_conversations'  ? list_conversations
: $conversation                     ? continue_conversation
                                    : answer;

__END__

=head1 NAME

groq - retrieve chat completions from the Groq API

=head1 SYNOPSIS

 groq [options]

 Options:
   --help        | -h  Display this help message
   --query       | -q  Prompt to send to the API (required)
   --model       | -m  Model to use (default: llama-3.3-70b-versatile)
   --rounds      | -r  Number of rounds of thinking (default: 3)
   --file        | -f  Include a file's contents in your query; can be used multiple times
   --save        | -s  Save the conversation to a file
   --cont        | -c  Continue a saved conversation
   --conversations     List saved conversations
   --models            List available models

=head1 DESCRIPTION

This script uses the Groq API to generate chat completions.
It uses chain-of-thought reasoning to improve the quality of its responses.

=head1 REQUIREMENTS

This script requires Perl v5.14 or later.
You must set C<GROQ_API_KEY> to your L<Groq API key|https://console.groq.com/keys> in your shell.

=head1 OPTIONS

=over

=item B<--help>, B<-h>

Display this help message.

=item B<--models>

List available models.
Ignores all other options (except C<--help>).

=item B<--conversations>

List saved conversations.
Ignores all other options (except C<--help>).

=item B<--query>, B<-q>

The prompt to send to the API.

=item B<--save>, B<-s>

Save the conversation to continue it later.
Conversations are saved to C<$HOME/.groq/conversations>.

=item B<--cont>, B<-c>

Continue a saved conversation.
List saved conversations with C<--conversations>.

=item B<--model>, B<-m>

The model to use.
Defaults to C<llama-3.3-70b-specdec>.
Use C<--models> to see available models.

=item B<--rounds>, B<-r>

The number of rounds of reasoning to perform.
Defaults to 3.

Each round requires an additional API call.
"Reasoning" responses are limited to 2,000 tokens.
In general, 3-5 rounds of reasoning is sufficient for most cases.

=item B<--file>, B<-f>

Include a file's contents in your query.
Can be used multiple times.

=back

=head1 OUTPUT

If L<glow|https://github.com/charmbracelet/glow> is installed, it will be used for format the output.
Otherwise, the output will be markdown-formatted text.

Well, I<probably> markdown-formatted text. The LLM may have other ideas.

=head2 REDIRECTED I/O

Alternately, you can pipe a prompt into the script:

  echo "What is the meaning of life?" | groq

C<groq> will generate a response for each line of input:

  echo -e "What is the meaning of life?\nWhat is the airspeed velocity of an unladen swallow?" | groq

=head2 REASONING

This script will perform C<rounds> rounds of reasoning before responding to the query.
Reasoning steps are emitted to C<STDERR>, while the final response is printed to C<STDOUT>.

=head1 AUTHOR

Jeff Ober

=head1 COPYRIGHT

Copyright (C) 2025 Jeff Ober

=head1 LICENSE

This program is free software: you can redistribute it and/or modify it it under the terms of the L<MIT License|https://github.com/sysread/groq.pl/LICENSE>.

=cut
